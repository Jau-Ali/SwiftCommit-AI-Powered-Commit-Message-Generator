{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67602d1-e712-4f14-8824-c8b6865d4a4c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size:20px; color:Green;\">\n",
    "    Transformer Based Model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77303b4-07bd-4b05-a2a2-7a7e9ce443af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CommitBench dataset\n",
    "commitbench_df = pd.read_csv('C:/Users/salij/Desktop/THESIS/commitbench.csv')\n",
    "\n",
    "# Extract necessary columns for training ('diff' as input, 'message' as target)\n",
    "commitbench_df = commitbench_df[['diff', 'message']]\n",
    "\n",
    "# Reduce the dataset to 1/8 of the original size\n",
    "commitbench_df = commitbench_df.sample(frac=0.125, random_state=42)\n",
    "\n",
    "# Split into training and validation sets (10% validation)\n",
    "train_bench, val_bench = train_test_split(commitbench_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Save preprocessed data\n",
    "train_bench.to_csv('train_bench.csv', index=False)\n",
    "val_bench.to_csv('val_bench.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727efde3-60f6-43e5-bd45-eb544d0c8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb691726-d5ea-482b-984d-e436cbdcfe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommitMessageDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, source_max_len, target_max_len):\n",
    "        self.diff = data['diff'].tolist()  # Extract 'diff' column as a list\n",
    "        self.message = data['message'].tolist()  # Extract 'message' column as a list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_len = source_max_len\n",
    "        self.target_max_len = target_max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.diff)  # Return the length of the dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.diff[index])\n",
    "        target_text = str(self.message[index])\n",
    "\n",
    "        # Tokenize the source and target text\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.target_max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'source_ids': source['input_ids'].squeeze(),\n",
    "            'source_mask': source['attention_mask'].squeeze(),\n",
    "            'target_ids': target['input_ids'].squeeze(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78914d0a-2168-4330-ba1b-27354bfc7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set max token lengths\n",
    "source_max_len = 128\n",
    "target_max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa480f1f-2de7-4e36-a294-66dd8b7b7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames into Dataset objects\n",
    "train_dataset = CommitMessageDataset(train_bench, tokenizer, source_max_len, target_max_len)\n",
    "val_dataset = CommitMessageDataset(val_bench, tokenizer, source_max_len, target_max_len)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e80d4fe5-b94b-45cb-9abf-363e6f5f0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salij\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['source_ids'].to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            target_ids = batch['target_ids'].to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb05bf2-60cb-41d8-9b66-33597fa154b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfe751-4258-48ca-8a40-015c7a8d92c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
